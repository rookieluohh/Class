{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73268bfb-ba0f-43cf-bcee-e79d863fa5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2312ac16-3ace-44bf-8b9c-917ada7ea009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=20),            # 随机旋转图像，范围是 [-20度, 20度]\n",
    "    transforms.RandomHorizontalFlip(p=0.5),           # 50%的概率水平翻转图像\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # 随机平移，最大平移10%\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))              # 标准化\n",
    "])\n",
    "\n",
    "# 获取 MNIST 数据集\n",
    "train_dataset = datasets.MNIST(root='/public/group_data_2023/luohh/Class/01.OmicsAndAI/01.Materials/homework/data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='/public/group_data_2023/luohh/Class/01.OmicsAndAI/01.Materials/homework/data', train=False, download=True, transform=transform)\n",
    "\n",
    "# 数据加载器\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52653a30-f20c-48db-8cd3-997e48424eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_conv_layers, kernel_size, num_channels):\n",
    "        super(CNN, self).__init__()\n",
    "        layers = []\n",
    "        input_channels = 1\n",
    "        \n",
    "        # 创建卷积层\n",
    "        for _ in range(num_conv_layers):\n",
    "            layers.append(nn.Conv2d(input_channels, num_channels, kernel_size=kernel_size, padding=kernel_size//2))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.MaxPool2d(2))  # 每个卷积后进行池化\n",
    "            input_channels = num_channels\n",
    "            \n",
    "        self.features = nn.Sequential(*layers)\n",
    "        \n",
    "        # 计算卷积层之后尺寸的变化\n",
    "        # 初始尺寸 28x28\n",
    "        # 每个卷积层后面跟一个 2x2 的池化层，假设 kernel_size 和 padding 不会影响尺寸\n",
    "        output_height = 28  # 每个卷积层后会减少 kernel_size 尺寸再进行池化\n",
    "        for _ in range(num_conv_layers):\n",
    "            output_height = (output_height - 2) // 2 + 1\n",
    "        \n",
    "        output_width = output_height  # MNIST 是正方形\n",
    "\n",
    "        self.fc = nn.Linear(num_channels * output_height * output_width, 10)  # 确保这里的计算正确\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # 展平\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0e77541-6443-4830-beae-d4f5c98144de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数范围\n",
    "# num_conv_layers_options = [1, 2, 3]\n",
    "num_conv_layers_options = [3]\n",
    "# kernel_size_options = [3, 5, 7]\n",
    "kernel_size_options = [5]\n",
    "# num_channels_options = [4, 8, 16, 32]\n",
    "num_channels_options = [32]\n",
    "# learning_rate_options = [0.01, 0.0001, 0.00001]\n",
    "learning_rate_options = [0.0001]\n",
    "# epochs_options = range(1, 11)\n",
    "epochs_options = [10]\n",
    "batch_size_options = [16, 32, 64, 128]\n",
    "# batch_size_options = [64]\n",
    "\n",
    "results = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfdc6b24-4ce8-4488-ad12-6b611c447947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### num_conv_layers = 3, kernel_size = 5, num_channels = 32, learning_rate = 0.0001, epochs = 10, batch_size = 16\n",
      "Accuracy =  0.9595\n",
      "### num_conv_layers = 3, kernel_size = 5, num_channels = 32, learning_rate = 0.0001, epochs = 10, batch_size = 32\n",
      "Accuracy =  0.9522\n",
      "### num_conv_layers = 3, kernel_size = 5, num_channels = 32, learning_rate = 0.0001, epochs = 10, batch_size = 64\n",
      "Accuracy =  0.9328\n",
      "### num_conv_layers = 3, kernel_size = 5, num_channels = 32, learning_rate = 0.0001, epochs = 10, batch_size = 128\n",
      "Accuracy =  0.9145\n"
     ]
    }
   ],
   "source": [
    "# 遍历超参数组合\n",
    "for num_conv_layers in num_conv_layers_options:\n",
    "    for kernel_size in kernel_size_options:\n",
    "        for num_channels in num_channels_options:\n",
    "            for learning_rate in learning_rate_options:\n",
    "                for epochs in epochs_options:\n",
    "                    for batch_size in batch_size_options:\n",
    "                        # 更新数据加载器的batch_size\n",
    "                        print(\"### num_conv_layers = \"+str(num_conv_layers)+\", kernel_size = \"+str(kernel_size)+\", num_channels = \"+str(num_channels)+\n",
    "                              \", learning_rate = \"+str(learning_rate)+\", epochs = \"+str(epochs)+\", batch_size = \"+str(batch_size))\n",
    "                        train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                        test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                        model = CNN(num_conv_layers, kernel_size, num_channels).to(device)\n",
    "                        criterion = nn.CrossEntropyLoss()\n",
    "                        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "                        # 训练模型\n",
    "                        model.train()\n",
    "                        for epoch in range(epochs):\n",
    "                            for images, labels in train_loader:\n",
    "                                images, labels = images.to(device), labels.to(device)\n",
    "                                optimizer.zero_grad()\n",
    "                                outputs = model(images)\n",
    "                                loss = criterion(outputs, labels)\n",
    "                                loss.backward()\n",
    "                                optimizer.step()\n",
    "\n",
    "                        # 评估模型\n",
    "                        model.eval()\n",
    "                        correct = 0\n",
    "                        total = 0\n",
    "                        with torch.no_grad():\n",
    "                            for images, labels in test_loader:\n",
    "                                images, labels = images.to(device), labels.to(device)\n",
    "                                outputs = model(images)\n",
    "                                _, predicted = torch.max(outputs.data, 1)\n",
    "                                total += labels.size(0)\n",
    "                                correct += (predicted == labels).sum().item()\n",
    "                        \n",
    "                        test_accuracy = correct / total\n",
    "                        results.append((num_conv_layers, kernel_size, num_channels, learning_rate, epochs, batch_size, test_accuracy))\n",
    "                        print(\"Accuracy = \",test_accuracy)\n",
    "                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
